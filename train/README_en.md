*[中文README](README.md).*

# Usage

This repository is used to fine-tune the Bloom and Llama large language models, and supports LoRA training.

## Environment Setup

```bash
conda env create -f environment.yml
conda activate Belle
conda install -c nvidia libcusolver-dev
```

## Data Download

```bash
python download_data.py
```

Create the `data_dir` folder and download the Chinese dataset [1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) + [0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) generated by [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca). The training and testing data are randomly split.

## Model Training

The model training configuration files are stored in the `run_config` folder.

- `Bloom_config.json`: hyperparameters for the Bloom model
- `Llama_config.json`: hyperparameters for the Llama model
- `deepspeed_config.json`: parameters for the Deepspeed strategy
- `lora_hyperparams_bloom.json`: parameters for training the Bloom model with LoRA 
- `lora_hyperparams_llama.json`: parameters for training the Llama model with LoRA 


Command to train the Bloom model:

```bash
deepspeed --num_gpus=8 finetune.py --model_config_file run_config/Bloom_config.json  --deepspeed run_config/deepspeed_config.json 
```

Command to train the Llama model:

```bash
deepspeed --num_gpus=8 finetune.py --model_config_file run_config/Llama_config.json  --deepspeed run_config/deepspeed_config.json 
```


### Lora

If using LoRA, start the distributed training using the `torchrun` command (an error will occur if starting with `deepspeed`). Also, the `use_lora` parameter needs to be specified, and the `lora_hyperparams_file` file that LoRA needs should be provided.

Command to train using LoRA(Bloom):

```bash
torchrun --nproc_per_node=8 finetune.py --model_config_file run_config/Bloom_config.json --lora_hyperparams_file run_config/lora_hyperparams_bloom.json  --use_lora
```

Command to train using LoRA(Llama):

```bash
torchrun --nproc_per_node=8 finetune.py --model_config_file run_config/Llama_config.json --lora_hyperparams_file run_config/lora_hyperparams_llama.json  --use_lora
```

## Text Generation

The trained models will be saved in the `trained_models/model_name` directory, where `model_name` is the name of the model, such as Bloom or Llama. Assuming that the trained model is Bloom and the training data used is Belle_open_source_0.5M, the following command will read the model and generate the results for each sample in the test set.

```bash
python generate.py --dev_file data_dir/Belle_open_source_0.5M.dev.json --model_name_or_path trained_models/bloom/
```

If it is a LoRA model, the location where the LoRA weights are saved needs to be provided, such as `--lora_weights trained_models/lora-llama`.

## Reference

The code in this repository is based on [alpaca-lora](https://github.com/tloen/alpaca-lora).
